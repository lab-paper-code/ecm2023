{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 17:27:45.352464: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "from scipy.interpolate import interp1d\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization,LSTM,GRU,Dropout,Dense, SimpleRNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import datetime\n",
    "import csv\n",
    "import os\n",
    "import datetime\n",
    "warnings.filterwarnings(action='ignore')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dfh : hvac data\n",
    "dfs : sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_feather('/home/deallab/harin/ECM/ecm_input.ftr')\n",
    "df_train = df_train.set_index('timestamp_min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f3_hum</th>\n",
       "      <th>f10_hum</th>\n",
       "      <th>f3_temp</th>\n",
       "      <th>f10_temp</th>\n",
       "      <th>exthum</th>\n",
       "      <th>enhum</th>\n",
       "      <th>temp</th>\n",
       "      <th>entemp</th>\n",
       "      <th>extemp</th>\n",
       "      <th>F_SET_MODE_0</th>\n",
       "      <th>F_SET_MODE_1</th>\n",
       "      <th>F_SET_MODE_2</th>\n",
       "      <th>F_SET_MODE_3</th>\n",
       "      <th>F_SET_MODE_4</th>\n",
       "      <th>F_SET_MODE_5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp_min</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-08-21 16:37:00</th>\n",
       "      <td>57.37</td>\n",
       "      <td>51.87</td>\n",
       "      <td>25.47</td>\n",
       "      <td>26.77</td>\n",
       "      <td>59.6</td>\n",
       "      <td>85.6</td>\n",
       "      <td>81.5</td>\n",
       "      <td>29.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-21 16:38:00</th>\n",
       "      <td>57.37</td>\n",
       "      <td>51.87</td>\n",
       "      <td>25.47</td>\n",
       "      <td>26.77</td>\n",
       "      <td>59.4</td>\n",
       "      <td>85.4</td>\n",
       "      <td>107.6</td>\n",
       "      <td>29.2</td>\n",
       "      <td>27.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-21 16:39:00</th>\n",
       "      <td>57.27</td>\n",
       "      <td>51.85</td>\n",
       "      <td>25.50</td>\n",
       "      <td>26.77</td>\n",
       "      <td>57.3</td>\n",
       "      <td>85.4</td>\n",
       "      <td>105.2</td>\n",
       "      <td>29.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-21 16:40:00</th>\n",
       "      <td>57.27</td>\n",
       "      <td>51.87</td>\n",
       "      <td>25.60</td>\n",
       "      <td>26.77</td>\n",
       "      <td>58.3</td>\n",
       "      <td>85.6</td>\n",
       "      <td>82.5</td>\n",
       "      <td>29.2</td>\n",
       "      <td>27.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-21 16:41:00</th>\n",
       "      <td>57.20</td>\n",
       "      <td>51.85</td>\n",
       "      <td>25.60</td>\n",
       "      <td>26.77</td>\n",
       "      <td>59.5</td>\n",
       "      <td>85.8</td>\n",
       "      <td>76.9</td>\n",
       "      <td>29.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-24 23:55:00</th>\n",
       "      <td>36.72</td>\n",
       "      <td>40.12</td>\n",
       "      <td>23.62</td>\n",
       "      <td>22.32</td>\n",
       "      <td>52.6</td>\n",
       "      <td>83.1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>19.9</td>\n",
       "      <td>20.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-24 23:56:00</th>\n",
       "      <td>36.72</td>\n",
       "      <td>40.10</td>\n",
       "      <td>23.62</td>\n",
       "      <td>22.42</td>\n",
       "      <td>52.8</td>\n",
       "      <td>82.8</td>\n",
       "      <td>40.0</td>\n",
       "      <td>19.9</td>\n",
       "      <td>20.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-24 23:57:00</th>\n",
       "      <td>36.85</td>\n",
       "      <td>40.12</td>\n",
       "      <td>23.62</td>\n",
       "      <td>22.42</td>\n",
       "      <td>52.8</td>\n",
       "      <td>83.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>19.9</td>\n",
       "      <td>20.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-24 23:58:00</th>\n",
       "      <td>36.72</td>\n",
       "      <td>39.95</td>\n",
       "      <td>23.62</td>\n",
       "      <td>22.42</td>\n",
       "      <td>52.9</td>\n",
       "      <td>83.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>19.9</td>\n",
       "      <td>20.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-24 23:59:00</th>\n",
       "      <td>36.80</td>\n",
       "      <td>40.02</td>\n",
       "      <td>23.62</td>\n",
       "      <td>22.42</td>\n",
       "      <td>52.9</td>\n",
       "      <td>83.1</td>\n",
       "      <td>39.9</td>\n",
       "      <td>19.9</td>\n",
       "      <td>20.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91652 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     f3_hum  f10_hum  f3_temp  f10_temp  exthum  enhum   temp  \\\n",
       "timestamp_min                                                                   \n",
       "2019-08-21 16:37:00   57.37    51.87    25.47     26.77    59.6   85.6   81.5   \n",
       "2019-08-21 16:38:00   57.37    51.87    25.47     26.77    59.4   85.4  107.6   \n",
       "2019-08-21 16:39:00   57.27    51.85    25.50     26.77    57.3   85.4  105.2   \n",
       "2019-08-21 16:40:00   57.27    51.87    25.60     26.77    58.3   85.6   82.5   \n",
       "2019-08-21 16:41:00   57.20    51.85    25.60     26.77    59.5   85.8   76.9   \n",
       "...                     ...      ...      ...       ...     ...    ...    ...   \n",
       "2019-10-24 23:55:00   36.72    40.12    23.62     22.32    52.6   83.1   40.0   \n",
       "2019-10-24 23:56:00   36.72    40.10    23.62     22.42    52.8   82.8   40.0   \n",
       "2019-10-24 23:57:00   36.85    40.12    23.62     22.42    52.8   83.0   40.0   \n",
       "2019-10-24 23:58:00   36.72    39.95    23.62     22.42    52.9   83.0   40.0   \n",
       "2019-10-24 23:59:00   36.80    40.02    23.62     22.42    52.9   83.1   39.9   \n",
       "\n",
       "                     entemp  extemp  F_SET_MODE_0  F_SET_MODE_1  F_SET_MODE_2  \\\n",
       "timestamp_min                                                                   \n",
       "2019-08-21 16:37:00    29.2    27.3             0             0             0   \n",
       "2019-08-21 16:38:00    29.2    27.1             0             0             0   \n",
       "2019-08-21 16:39:00    29.2    27.3             0             0             0   \n",
       "2019-08-21 16:40:00    29.2    27.5             0             0             0   \n",
       "2019-08-21 16:41:00    29.2    27.3             0             0             0   \n",
       "...                     ...     ...           ...           ...           ...   \n",
       "2019-10-24 23:55:00    19.9    20.8             1             0             0   \n",
       "2019-10-24 23:56:00    19.9    20.8             1             0             0   \n",
       "2019-10-24 23:57:00    19.9    20.8             1             0             0   \n",
       "2019-10-24 23:58:00    19.9    20.8             1             0             0   \n",
       "2019-10-24 23:59:00    19.9    20.8             1             0             0   \n",
       "\n",
       "                     F_SET_MODE_3  F_SET_MODE_4  F_SET_MODE_5  \n",
       "timestamp_min                                                  \n",
       "2019-08-21 16:37:00             0             1             0  \n",
       "2019-08-21 16:38:00             0             1             0  \n",
       "2019-08-21 16:39:00             0             1             0  \n",
       "2019-08-21 16:40:00             0             1             0  \n",
       "2019-08-21 16:41:00             0             1             0  \n",
       "...                           ...           ...           ...  \n",
       "2019-10-24 23:55:00             0             0             0  \n",
       "2019-10-24 23:56:00             0             0             0  \n",
       "2019-10-24 23:57:00             0             0             0  \n",
       "2019-10-24 23:58:00             0             0             0  \n",
       "2019-10-24 23:59:00             0             0             0  \n",
       "\n",
       "[91652 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature = ['f3_hum','f10_hum','f3_temp','f10_temp','exthum','enhum','temp','entemp','extemp','F_SET_MODE_0','F_SET_MODE_1','F_SET_MODE_2','F_SET_MODE_3','F_SET_MODE_4','F_SET_MODE_5']\n",
    "output_feature = ['f3_hum','f10_hum','f3_temp','f10_temp','exthum','temp','extemp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100 #int\n",
    "steps_per_epoch = 200 #int\n",
    "BATCH_SIZE = 32 #int\n",
    "BUFFER_SIZE = 1000000 #int\n",
    "PATIENCE = 5 #i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 Timewindow별 학습하기 위한 데이터를 나눔\n",
    "\n",
    "\n",
    "def train_model_data(df_s, Train_windows_size, Train_date ):\n",
    "    \n",
    "    df_train = df_s[((pd.to_datetime(df_s.index).date) >= (Train_date + datetime.timedelta(days=-Train_windows_size))) & \n",
    "           ((pd.to_datetime(df_s.index).date) < Train_date)]\n",
    "    \n",
    "    df_test = df_s[(pd.to_datetime(df_s.index).date) == (Train_date)]\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_1layer_model():\n",
    "    model = Sequential()\n",
    "    model.add(GRU(128,input_shape = (1,len(input_feature))))\n",
    "    model.add(Dropout(0.30))\n",
    "    #model.add(Dropout(0.30))\n",
    "    #model.add(Dense(32, activation='relu'))\n",
    "    #model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(len(output_feature),activation = 'relu'))\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    model.compile(loss = \"mae\", optimizer = optimizer)#, metrics = ['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_1layer_model():\n",
    "    model = Sequential()\n",
    "    #model.add(BatchNormalization(input_shape = (1,len(input_feature))))\n",
    "    model.add(LSTM(128,input_shape = (1,len(input_feature))))\n",
    "    model.add(Dropout(0.30))\n",
    "    #model.add(Dropout(0.30))\n",
    "    #model.add(Dense(32, activation='relu'))\n",
    "    #model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(len(output_feature),activation = 'relu'))\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    model.compile(loss = \"mae\", optimizer = optimizer)#, metrics = ['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_rnn_1layer_model():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(128,input_shape = (1,len(input_feature))))\n",
    "    model.add(Dropout(0.30))\n",
    "    #model.add(Dropout(0.30))\n",
    "    #model.add(Dense(32, activation='relu'))\n",
    "    #model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(len(output_feature),activation = 'relu'))\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    model.compile(loss = \"mae\", optimizer = optimizer)#, metrics = ['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 및 결과 함수\n",
    "\n",
    "def train_models(model_name, df_pre, Shift_Target, df_test_pre):\n",
    "    \n",
    "   \n",
    "    if model_name == \"gru_1layer\":\n",
    "        model = build_gru_1layer_model()\n",
    "    elif model_name == \"lstm_1layer\":\n",
    "        model = build_lstm_1layer_model()\n",
    "    elif model_name == \"simpleRNN\":\n",
    "        model = build_simple_rnn_1layer_model()\n",
    "        \n",
    "    x_train = df_pre[input_feature][:-10].values\n",
    "    x_train  = x_train.reshape(-1,1,len(input_feature))\n",
    "    y_train = df_pre[output_feature][10:].values\n",
    "    x_test = df_test_pre[input_feature][:-10].values\n",
    "    x_test  = x_test.reshape(-1,1,len(input_feature))\n",
    "    y_test = df_test_pre[output_feature][10:].values\n",
    "    \n",
    "\n",
    "    model.fit(    x = x_train, \n",
    "                  y= y_train,\n",
    "                    epochs = EPOCHS,\n",
    "                    validation_split = 0.1,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_steps=steps_per_epoch,\n",
    "                    verbose=1,\n",
    "        #callbacks=[early_stopping])\n",
    "                 )\n",
    "    y_pred = model.predict(x_test)\n",
    "    #x_train, y_train =  multivariate_data(df_pre[input_feature].reset_index(drop=True).values, df_pre[output_feature].reset_index(drop=True).values, 0, None, past_history, future_target)\n",
    "    #x_test, y_test =  multivariate_data(df_test_pre[input_feature].reset_index(drop=True).values, df_test_pre[output_feature].reset_index(drop=True).values, 0, None, past_history, future_target)\n",
    "    #x_train = x_train.reshape(-1,past_history*len(input_feature))\n",
    "    #x_test = x_test.reshape(-1,past_history*len(input_feature))\n",
    "   \n",
    "   \n",
    "    #model.fit(df_pre[:-Shift_Target], df_pre[Shift_Target:])\n",
    "    \n",
    "    \n",
    "    \n",
    "    mae = mean_absolute_error(y_pred[:,2], y_test[:,2])\n",
    "    rmse = mean_squared_error(y_pred[:,2], y_test[:,2], squared=False)\n",
    "    r2 = r2_score(y_pred[:,2], y_test[:,2])\n",
    "    mape = mean_absolute_percentage_error(y_pred[:,2], y_test[:,2])\n",
    "    \n",
    "#     print(f\"MAE : \" , mae)\n",
    "#     print(\"RMSE: \", rmse)\n",
    "#     print(\"R2: \", r2)\n",
    "#     print(\"MAPE: \", mape)\n",
    "    \n",
    "    h_mae = mean_absolute_error(y_pred[:,0], y_test[:,0])\n",
    "    h_rmse = mean_squared_error(y_pred[:,0], y_test[:,0], squared=False)\n",
    "    h_r2 = r2_score(y_pred[:,0], y_test[:,0])\n",
    "    h_mape = mean_absolute_percentage_error(y_pred[:,0], y_test[:,0])\n",
    "    \n",
    "#     print(f\"h_MAE : \" , h_mae)\n",
    "#     print(\"h_RMSE: \", h_rmse)\n",
    "#     print(\"h_R2: \", h_r2)\n",
    "#     print(\"h_MAPE: \", h_mape)\n",
    "    \n",
    "    return model, y_pred, [mae, rmse, r2, mape, h_mae, h_rmse, h_r2, h_mape] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 결과 값 및 모델 저장\n",
    "def model_save(Train_date, Train_windows_size, model_name, export_path, model, model_error ):\n",
    "    \n",
    "    model_filename = Train_date.strftime(\"%Y%m%d\") + \"w\" + str(Train_windows_size) + \"_\" + model_name + \".h5\"\n",
    "    \n",
    "    filepath = os.path.join(export_path,model_filename)\n",
    "    \n",
    "    # extra_trees_regressor 모델 값이 너무 커서 저장하지 않음.\n",
    "    # 향후 풀어야되는 문제임\n",
    "    #if not model_name == \"extra_trees_regressor\":\n",
    "    if not os.path.exists(f\"{export_path}\"):\n",
    "        os.makedirs(f\"{export_path}\", exist_ok=True)\n",
    "    if not os.path.isfile(filepath):\n",
    "        model.save(filepath)\n",
    "    print(f\"{model_filename} 에 DL 모델이 저장되었습니다.\")\n",
    "\n",
    "\n",
    "\n",
    "    #엑셀(csv) 저장\n",
    "    f = open(csv_name,'a+', newline='')\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([Train_date.strftime(\"%Y-%m-%d\"), model_name, str(Train_windows_size), \n",
    "              model_error[0],model_error[1],model_error[2],model_error[3],\n",
    "              model_error[4],model_error[5],model_error[6],model_error[7]])\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid/deallab/ecm/model_result_dl/2019-08-23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 17:29:00.633510: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2023-01-31 17:29:00.685837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:08:00.0 name: Tesla V100-DGXS-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-01-31 17:29:00.685890: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-01-31 17:29:00.688347: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-01-31 17:29:00.690313: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2023-01-31 17:29:00.690667: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2023-01-31 17:29:00.693111: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-01-31 17:29:00.694390: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-01-31 17:29:00.699117: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-01-31 17:29:00.703713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2023-01-31 17:29:00.704122: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-31 17:29:00.717465: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2198805000 Hz\n",
      "2023-01-31 17:29:00.720023: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563ff28e7590 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-31 17:29:00.720057: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-01-31 17:29:00.895514: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563ff2054320 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-31 17:29:00.895561: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-DGXS-32GB, Compute Capability 7.0\n",
      "2023-01-31 17:29:00.899895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:08:00.0 name: Tesla V100-DGXS-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-01-31 17:29:00.899979: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-01-31 17:29:00.900026: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-01-31 17:29:00.900065: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2023-01-31 17:29:00.900109: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2023-01-31 17:29:00.900147: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-01-31 17:29:00.900185: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-01-31 17:29:00.900224: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-01-31 17:29:00.913455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2023-01-31 17:29:00.913513: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-01-31 17:29:01.398665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-31 17:29:01.398699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2023-01-31 17:29:01.398707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2023-01-31 17:29:01.402812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30131 MB memory) -> physical GPU (device: 0, name: Tesla V100-DGXS-32GB, pci bus id: 0000:08:00.0, compute capability: 7.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 17:29:03.759028: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-01-31 17:29:04.019594: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - ETA: 0s - loss: 35.5213WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 35.5213 - val_loss: 23.2826\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 17.9694\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 8.5576\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 7.0870\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 6.4376\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.9304\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.6972\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.6992\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.5740\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.4337\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.3764\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.2110\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.1983\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.1162\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.0751\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.0376\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.9024\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.7772\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.6494\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.6675\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.5720\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.4297\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.3667\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.2819\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.3004\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.2534\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.1475\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.1384\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 4.0215\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.9916\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.0853\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.9996\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.0298\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.9786\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.9100\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8686\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8979\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.9022\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.0400\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8042\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7937\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7978\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.9425\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.9015\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8869\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7790\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8852\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8380\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8054\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8883\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8107\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7648\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8218\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8040\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8611\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7952\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8179\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.8152\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6796\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8447\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.6887\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.6405\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8408\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8149\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7082\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7361\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7700\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7051\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7444\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7292\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7245\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7267\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6978\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7620\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6917\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7231\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7900\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7259\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6759\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7359\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6769\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6189\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7120\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.5840\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6042\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7121\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7366\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7423\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6210\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6678\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.5659\n",
      "Epoch 92/100\n",
      " 90/200 [============>.................] - ETA: 0s - loss: 3.8001WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/200 [==============>...............] - 0s 3ms/step - loss: 3.7691\n",
      "20190823w1_gru_1layer.h5 에 DL 모델이 저장되었습니다.\n",
      "Epoch 1/100\n",
      "191/200 [===========================>..] - ETA: 0s - loss: 38.5565WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 38.1637 - val_loss: 27.6714\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 22.0604\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 11.7334\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 7.5776\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 6.8235\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 6.2475\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.8778\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.6211\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.5617\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.4829\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.2891\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.3515\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.1219\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.0628\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.9779\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.9749\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 4.8471\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 4.7628\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 4.6563\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 4.6238\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 4.5501\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 4.3900\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 4.3417\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 4.3381\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 4.3461\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 4.2544\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 4.2668\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 4.2407\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 4.1350\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.1469\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.0957\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.0622\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.0407\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.0443\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.0015\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.0635\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.9542\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.0084\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.0046\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7862\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.9714\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.9042\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8592\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8815\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.8766\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.8520\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.8240\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.9221\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8969\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7924\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7869\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8328\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8910\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.7694\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7389\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8799\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 4.0486\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.9876\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.9532\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8849\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.8677\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.8666\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7473\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8469\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7738\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8025\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8361\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8071\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7915\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7745\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7757\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7319\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7248\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7796\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6603\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7000\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7857\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7096\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7285\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6242\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6275\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6917\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6272\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6777\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7231\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6302\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.6895\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.6452\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.7566\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.6397\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.6555\n",
      "Epoch 92/100\n",
      " 90/200 [============>.................] - ETA: 0s - loss: 3.6327WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/200 [==============>...............] - 0s 4ms/step - loss: 3.6088\n",
      "20190823w1_lstm_1layer.h5 에 DL 모델이 저장되었습니다.\n",
      "Epoch 1/100\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 33.4557WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 33.0034 - val_loss: 19.7706\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 13.8315\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 7.3667\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 6.5754\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 6.0649\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.9082\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.8384\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.8258\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.7703\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.6060\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.5273\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.5847\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.3537\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.3601\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.2214\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.2129\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.0998\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.0416\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.0370\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.8445\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.7959\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.7383\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.6528\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.6017\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.5194\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.3994\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.3377\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.3175\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.2433\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.1421\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.0864\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.0460\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.9575\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.9550\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.9321\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.9053\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7733\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8895\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8572\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7649\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8405\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7171\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7197\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7779\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8356\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7925\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7265\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7601\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7424\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6996\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7429\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6800\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7502\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8517\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7738\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7934\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6586\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6906\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7925\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8336\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7448\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7759\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.8201\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7358\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7374\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6832\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7011\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7455\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7074\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7019\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7141\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6579\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6629\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7925\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7042\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6795\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6902\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7462\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7393\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6908\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6922\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6802\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7014\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6917\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7558\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7408\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7000\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6789\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7006\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7310\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6994\n",
      "Epoch 92/100\n",
      " 96/200 [=============>................] - ETA: 0s - loss: 3.7953WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/200 [==============>...............] - 0s 3ms/step - loss: 3.7859\n",
      "20190823w1_simpleRNN.h5 에 DL 모델이 저장되었습니다.\n",
      "/raid/deallab/ecm/model_result_dl/2019-08-24\n",
      "Epoch 1/100\n",
      "191/200 [===========================>..] - ETA: 0s - loss: 33.8476WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 33.3816 - val_loss: 23.3361\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 15.3064\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 6.3004\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2096\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5302\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5638\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4633\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5280\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5029\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4922\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5133\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4554\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.5964\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4825\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4512\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4609\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4904\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5300\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4903\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3954\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5268\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4096\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3653\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4493\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4162\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3614\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3823\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3975\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4532\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4614\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4015\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3989\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4205\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3984\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4124\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4196\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3381\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3589\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3310\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3583\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3265\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3406\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3453\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3194\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2969\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2944\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3353\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2851\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3002\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2780\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3538\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3045\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3186\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2291\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3105\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3090\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2696\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3082\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3803\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3025\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2892\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2152\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2588\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3045\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2760\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2684\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2798\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2083\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2433\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2230\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1787\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2696\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2744\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2965\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2581\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2404\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2002\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2414\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2241\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2648\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2723\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2971\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2514\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1854\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2567\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2842\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2057\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2532\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2109\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2884\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2320\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2740\n",
      "Epoch 93/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0/200 [..............................] - 0s 0s/step - loss: 2.2740\n",
      "20190824w1_gru_1layer.h5 에 DL 모델이 저장되었습니다.\n",
      "Epoch 1/100\n",
      "199/200 [============================>.] - ETA: 0s - loss: 35.0338WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 34.9892 - val_loss: 26.9502\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 20.5660\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 11.4029\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 6.2959\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.9497\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.6288\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.5756\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5402\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5378\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5314\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.6167\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.6210\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5808\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4839\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4277\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5375\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5550\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5461\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4893\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5532\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4820\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4813\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4657\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4950\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4792\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3509\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4876\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4617\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4712\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4718\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4688\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3892\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4242\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4026\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3992\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3994\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4074\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3794\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4059\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3819\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4296\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3386\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3573\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5061\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4168\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4282\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4318\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4347\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3677\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3316\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3351\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3181\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3898\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3805\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3846\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3642\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3801\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3749\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3224\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.4221\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3409\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.4243\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.4728\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2983\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.4766\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3495\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3706\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3832\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3594\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3575\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3366\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3846\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.4040\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3766\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2857\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3157\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3664\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3687\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3439\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3668\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3560\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3892\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3496\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3580\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3025\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3350\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3832\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3732\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3648\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3954\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4211\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3726\n",
      "Epoch 93/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0/200 [..............................] - 0s 0s/step - loss: 2.3726\n",
      "20190824w1_lstm_1layer.h5 에 DL 모델이 저장되었습니다.\n",
      "Epoch 1/100\n",
      "188/200 [===========================>..] - ETA: 0s - loss: 30.9028WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 30.2017 - val_loss: 18.8558\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 12.0685\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.8626\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5125\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3685\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3665\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4064\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3844\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3693\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4096\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3578\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3942\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3060\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3193\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3069\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3413\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3219\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3004\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3019\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2943\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2861\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2874\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2157\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2743\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3217\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3163\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2529\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3545\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2487\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2343\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2851\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2997\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2810\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2648\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2352\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2287\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3118\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2691\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2613\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2002\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2733\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2489\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2183\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3595\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2754\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1732\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2317\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1952\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2904\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2362\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2251\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2679\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2685\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2471\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2628\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2019\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2184\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2563\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2310\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2773\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2583\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1772\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2314\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1942\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2639\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2377\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2371\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1987\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2298\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2990\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2161\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2076\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2037\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1858\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2039\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2481\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2242\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2150\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2355\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2128\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1842\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2513\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1968\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2618\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2040\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1949\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1593\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2200\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2242\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2152\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2469\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2289\n",
      "Epoch 93/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0/200 [..............................] - 0s 0s/step - loss: 2.2289\n",
      "20190824w1_simpleRNN.h5 에 DL 모델이 저장되었습니다.\n",
      "/raid/deallab/ecm/model_result_dl/2019-08-25\n",
      "Epoch 1/100\n",
      "197/200 [============================>.] - ETA: 0s - loss: 30.5357WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 30.3756 - val_loss: 20.2011\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 13.3913\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.7535\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.9184\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5103\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3769\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4321\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3373\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4053\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3505\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3135\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3051\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2087\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2622\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2945\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3140\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3093\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3336\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2398\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2830\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2565\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2493\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2697\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2236\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1824\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2768\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2363\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1945\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1282\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1876\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1954\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1760\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1211\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1829\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1856\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2204\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2002\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1189\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1843\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1930\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2336\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1870\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1703\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1654\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1665\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1933\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1329\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1990\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2293\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2274\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2243\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1629\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1925\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1819\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2252\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2009\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2402\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1449\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1345\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1594\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2251\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1744\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1955\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0670\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1324\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1372\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2354\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1383\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1132\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1479\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0932\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1649\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1762\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1944\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1370\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1069\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1322\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1067\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1981\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0777\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0947\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1807\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1094\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1450\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0401\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1560\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1608\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1069\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0713\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1361\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0886\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1559\n",
      "Epoch 93/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0/200 [..............................] - 0s 0s/step - loss: 2.1559\n",
      "20190825w1_gru_1layer.h5 에 DL 모델이 저장되었습니다.\n",
      "Epoch 1/100\n",
      "190/200 [===========================>..] - ETA: 0s - loss: 33.4819WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 33.0794 - val_loss: 25.5867\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 19.7937\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 11.0267\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 6.4551\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.1705\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.7592\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.5136\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.5556\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.5459\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.5699\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.4637\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.5482\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.5080\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.4783\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4794\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5080\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4825\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3874\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5412\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4295\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4099\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3951\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3374\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4348\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.4071\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4871\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3964\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4179\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3517\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3729\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3876\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3127\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3828\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2966\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3064\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4394\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3335\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3635\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4196\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3727\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3799\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3129\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3073\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.4206\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3411\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2987\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3109\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3960\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2824\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3164\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3754\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3393\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3408\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.4241\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3963\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3423\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3610\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3780\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3202\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3040\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2985\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2517\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3078\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3357\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3091\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2532\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3281\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2813\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2932\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2417\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3279\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2901\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3100\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2703\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2920\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3653\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2647\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3468\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2846\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3095\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2916\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2746\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3006\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4068\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2541\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2299\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2588\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2735\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3530\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3139\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3006\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2644\n",
      "Epoch 93/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0/200 [..............................] - 0s 0s/step - loss: 2.2644\n",
      "20190825w1_lstm_1layer.h5 에 DL 모델이 저장되었습니다.\n",
      "Epoch 1/100\n",
      "200/200 [==============================] - ETA: 0s - loss: 28.8352WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 28.8352 - val_loss: 18.6246\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 11.3839\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.6899\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4437\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4314\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4489\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4224\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4268\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3811\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4135\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3326\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3540\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3628\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3903\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2942\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3262\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2841\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3705\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3213\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3256\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3531\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3707\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3659\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3030\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3549\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3804\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3482\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3010\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3201\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3032\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3198\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2530\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3001\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2970\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2879\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3410\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2867\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2951\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3305\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3174\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3424\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3048\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2680\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2917\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2892\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2779\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3150\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2828\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3100\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3118\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2864\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2074\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2812\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2034\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3119\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2975\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3181\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3405\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3103\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2791\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2937\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3570\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3082\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2294\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2620\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2709\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2503\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2298\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2615\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2851\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2824\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2938\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2034\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3602\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2495\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3023\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2692\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2765\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3094\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2776\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2016\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2790\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2547\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2335\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2344\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2842\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2056\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3147\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2944\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2715\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2666\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3153\n",
      "Epoch 93/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0/200 [..............................] - 0s 0s/step - loss: 2.3153\n",
      "20190825w1_simpleRNN.h5 에 DL 모델이 저장되었습니다.\n",
      "/raid/deallab/ecm/model_result_dl/2019-08-26\n",
      "Epoch 1/100\n",
      "186/200 [==========================>...] - ETA: 0s - loss: 29.5803WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 28.9265 - val_loss: 20.5392\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 13.9894\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 6.2242\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.0815\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2658\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2158\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2419\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1538\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2264\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1499\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1627\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1419\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1552\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1649\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1527\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1480\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1774\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1920\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1247\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1656\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2139\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1370\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1429\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0998\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1429\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0788\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1501\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0867\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1753\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0750\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0630\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0888\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0837\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1283\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0707\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0899\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1526\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0834\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1562\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1039\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0727\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0831\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1167\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0998\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1127\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.9752\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1041\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1493\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0239\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0702\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0712\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0920\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0557\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0406\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1190\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0655\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0465\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9979\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1147\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0918\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0376\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0555\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1107\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0248\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0075\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0581\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0317\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0801\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0797\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0600\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0333\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0356\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0273\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0141\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0359\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0406\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0795\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0264\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0339\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0185\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0624\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9887\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0171\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9754\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9883\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0212\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9712\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0345\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0464\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9541\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9980\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0060\n",
      "Epoch 93/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0/200 [..............................] - 0s 0s/step - loss: 2.0060\n",
      "20190826w1_gru_1layer.h5 에 DL 모델이 저장되었습니다.\n",
      "Epoch 1/100\n",
      "200/200 [==============================] - ETA: 0s - loss: 33.1415WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 33.1415 - val_loss: 26.2350\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 19.3267\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 10.1038\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.9052\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6805\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3765\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2318\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2218\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2066\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2746\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2018\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2260\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1986\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1738\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2693\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2675\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1719\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0944\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1673\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1895\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2004\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2074\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1461\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1616\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2374\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0881\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2327\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1932\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2050\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1879\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1611\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2792\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1734\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1471\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1122\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0859\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1775\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1206\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2346\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1549\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1480\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1331\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1018\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2177\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1287\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1963\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0880\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1575\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1360\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1840\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0864\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1158\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1131\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1389\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1406\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1670\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1381\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1666\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0834\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0868\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1244\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0902\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0794\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1429\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1083\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1704\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2046\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1118\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1299\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0968\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0998\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1320\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0873\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1762\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0348\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0975\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1361\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2283\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0879\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1350\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0982\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0629\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0302\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1093\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1340\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0507\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1502\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0983\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0356\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0858\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0799\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1021\n",
      "Epoch 93/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0/200 [..............................] - 0s 0s/step - loss: 2.1021\n",
      "20190826w1_lstm_1layer.h5 에 DL 모델이 저장되었습니다.\n",
      "Epoch 1/100\n",
      "199/200 [============================>.] - ETA: 0s - loss: 27.5705WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 27.5205 - val_loss: 18.3104\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 11.3143\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.8343\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2803\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0469\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0765\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0553\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0785\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0869\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9927\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0038\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0006\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9873\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0539\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0180\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0195\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9989\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9630\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9440\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0432\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9450\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9886\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9490\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9996\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8795\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9945\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0255\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9397\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9564\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9823\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9393\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9206\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9563\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9379\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0165\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9131\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0298\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0412\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9705\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9688\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9722\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9283\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9348\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0305\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9785\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9299\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9064\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9747\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9435\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9772\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9323\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9667\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9667\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9516\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0002\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9475\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9140\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9902\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9406\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9508\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9420\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8735\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9519\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9283\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8894\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9345\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9434\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9113\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9148\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9785\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8955\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9579\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9670\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9628\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8687\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9025\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8643\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9360\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8909\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9420\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9024\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8879\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9186\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9295\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9267\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8216\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8880\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8785\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8840\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9337\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9167\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9925\n",
      "Epoch 93/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0/200 [..............................] - 0s 0s/step - loss: 1.9925\n",
      "20190826w1_simpleRNN.h5 에 DL 모델이 저장되었습니다.\n",
      "/raid/deallab/ecm/model_result_dl/2019-08-27\n",
      "Epoch 1/100\n",
      "185/200 [==========================>...] - ETA: 0s - loss: 30.8627WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 30.1173 - val_loss: 20.1795\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 14.5858\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 6.6929\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.5880\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2238\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2222\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1074\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1750\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1858\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1536\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0438\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0662\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1818\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1410\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1161\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0743\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1624\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1098\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0656\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1402\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0729\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0906\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1638\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0628\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0731\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0759\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1155\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0766\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0568\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0906\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1016\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1238\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1364\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0872\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1373\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0122\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0700\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1477\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0987\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0418\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9804\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0886\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0241\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1021\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0495\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0871\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0355\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0293\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0391\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0214\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0578\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0034\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0465\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0789\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0629\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0721\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0481\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.9814\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0048\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1483\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0619\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0901\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1593\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0112\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0735\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0208\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0643\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9776\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0439\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0203\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0987\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9418\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0618\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9912\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0088\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0292\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9916\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9714\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0240\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0009\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9269\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0887\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9552\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0120\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9833\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0168\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0318\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9652\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0017\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9703\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9320\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9871\n",
      "Epoch 93/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0/200 [..............................] - 0s 0s/step - loss: 1.9871\n",
      "20190827w1_gru_1layer.h5 에 DL 모델이 저장되었습니다.\n",
      "Epoch 1/100\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 33.3731WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 33.0856 - val_loss: 25.8874\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 20.4876\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 12.2214\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 7.0754\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 4.7908\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.8179\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2725\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2287\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2625\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3078\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2792\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2158\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2156\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2217\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2394\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2102\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2142\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1956\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2201\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1980\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1732\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2549\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1326\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1850\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2638\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1184\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2133\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1540\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1908\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1616\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2124\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1345\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1419\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1258\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1539\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1689\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1852\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1293\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1901\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2150\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1373\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0818\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1909\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0853\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0232\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1560\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1342\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2043\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1632\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1399\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1246\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0673\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1113\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1021\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1083\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0980\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1119\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1778\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1259\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1093\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0903\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1003\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0682\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1175\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1113\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1188\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1150\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0708\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1460\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0516\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0581\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0961\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1433\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0883\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0511\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0627\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0368\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1360\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1382\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1317\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0059\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0696\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0235\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0309\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0834\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1215\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0702\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0361\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0196\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0411\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0659\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0991\n",
      "Epoch 93/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0/200 [..............................] - 0s 0s/step - loss: 2.0991\n",
      "20190827w1_lstm_1layer.h5 에 DL 모델이 저장되었습니다.\n",
      "Epoch 1/100\n",
      "185/200 [==========================>...] - ETA: 0s - loss: 28.6833WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 27.9036 - val_loss: 17.2856\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 11.2851\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.9137\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2192\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0304\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0169\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9875\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0182\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8847\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9613\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9986\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8731\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9402\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9514\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8880\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9113\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8895\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9189\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8767\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9019\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8545\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9216\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8772\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9069\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8859\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8569\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8730\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8986\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8523\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8484\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9093\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8618\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9316\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8846\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9060\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9321\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8645\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8747\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8585\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8379\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8755\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9258\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8754\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8189\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8861\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8474\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8634\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.7610\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8475\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.7978\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8302\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8297\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8699\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8038\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8690\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8675\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8044\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8679\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8232\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8310\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8324\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8744\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8264\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8401\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8890\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8612\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.7990\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8573\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8522\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.7911\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8847\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8017\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.7729\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8546\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8721\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8703\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8023\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8250\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8095\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8016\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8735\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8144\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.7683\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8138\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8276\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8525\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.7996\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8460\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.7958\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8410\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8272\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.7344\n",
      "Epoch 93/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0/200 [..............................] - 0s 0s/step - loss: 1.7344\n",
      "20190827w1_simpleRNN.h5 에 DL 모델이 저장되었습니다.\n",
      "/raid/deallab/ecm/model_result_dl/2019-08-28\n",
      "Epoch 1/100\n",
      "186/200 [==========================>...] - ETA: 0s - loss: 33.1413WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 32.3731 - val_loss: 21.8994\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 15.1080\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 8.5690\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 7.7495\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 7.2088\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 6.8145\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 6.4974\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 6.0164\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.5031\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.0874\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.6094\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.3254\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.0497\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.0130\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.5960\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7252\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.5977\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.5717\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.5266\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4747\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.5150\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.5016\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3831\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4578\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4274\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4416\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3298\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3551\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4464\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4069\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3891\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3815\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4275\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4362\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4207\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3498\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4002\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4407\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4016\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.5192\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3351\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4149\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4145\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3193\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2848\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4006\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3754\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3747\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3711\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3092\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3931\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2825\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3994\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3373\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4547\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2842\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4070\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3396\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3624\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4013\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3927\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3516\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4819\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3336\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3552\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4155\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3427\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3149\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2208\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3718\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2842\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3160\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3139\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2678\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2088\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2245\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2531\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3025\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.1998\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2659\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2604\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2388\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2107\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2644\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3569\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2786\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2334\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2999\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2356\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2645\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.3434\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2694\n",
      "Epoch 93/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0/200 [..............................] - 0s 0s/step - loss: 3.2694\n",
      "20190828w1_gru_1layer.h5 에 DL 모델이 저장되었습니다.\n",
      "Epoch 1/100\n",
      "191/200 [===========================>..] - ETA: 0s - loss: 35.5507WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 35.1684 - val_loss: 28.0319\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 21.4642\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 12.7378\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 8.5501\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 7.6441\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 7.1456\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 6.6699\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 6.2872\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.9422\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.5523\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.2467\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.0293\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.7154\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.5945\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.4089\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.2500\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.2321\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.9377\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.9724\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7800\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7903\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7439\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7267\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7674\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.5849\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7118\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.5794\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6989\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6064\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6485\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.7198\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6639\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.5994\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6170\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.5541\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6700\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.5529\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6264\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.6103\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.5035\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.6539\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.6160\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.5799\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.6475\n"
     ]
    }
   ],
   "source": [
    "#모델 결과 값 저장\n",
    "csv_name = \"ecm_dl.csv\"\n",
    "\n",
    "Shift_Target = 10\n",
    "\n",
    "#모델 실행\n",
    "model_list = [\"gru_1layer\", \"lstm_1layer\",'simpleRNN']\n",
    "\n",
    "# 실험 파라메터\n",
    "# Train Windows size\n",
    "# Train 시작 일자\n",
    "\n",
    "#Train_windows_size_list = [1,7,14,21,30] #날짜\n",
    "\n",
    "start_date = datetime.date(2019,8,23)\n",
    "end_date = datetime.date(2019,10,25)\n",
    "std  = datetime.date(2019,8,22)\n",
    "\n",
    "# 날짜 array 생성\n",
    "if (end_date-start_date).days == 0:\n",
    "    end_date = start_date + datetime.timedelta(days=1)\n",
    "\n",
    "Train_date_list = [start_date + datetime.timedelta(days=x) for x in range(0, (end_date-start_date).days)]\n",
    "\n",
    "# 모델 학습\n",
    "\n",
    "for Train_date in Train_date_list:\n",
    "    if Train_date - std < datetime.timedelta(days=7) : Train_windows_size_list = [1]\n",
    "    elif Train_date - std < datetime.timedelta(days=14) : Train_windows_size_list = [1,7]\n",
    "    elif Train_date - std < datetime.timedelta(days=30) : Train_windows_size_list = [1,7,14]\n",
    "    else : Train_windows_size_list = [1,7,14,30] #날짜\n",
    "        \n",
    "    export_path = os.path.join('/raid/deallab/ecm', \"model_result_dl\")\n",
    "    #export_path = os.path.join(os.getcwd(), \"input wind\")\n",
    "    export_path = os.path.join(export_path, Train_date.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    print(export_path)\n",
    "    \n",
    "    for Train_windows_size in Train_windows_size_list:\n",
    "\n",
    "        df_pre, df_test_pre = train_model_data(df_train, Train_windows_size, Train_date )\n",
    "\n",
    "        for m_name in model_list:\n",
    "\n",
    "            model, y_pred, model_error  = train_models(m_name, df_pre, Shift_Target, df_test_pre)\n",
    "            model_save(Train_date, Train_windows_size, m_name, export_path, model, model_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:daewoong]",
   "language": "python",
   "name": "conda-env-daewoong-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
